---
title: "spatialcluster"
author: "Mark Padgham"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: true
        number_sections: true
        theme: flatly
bibliography: spatialcluster.bib
header-includes: 
    - \usepackage{tikz}
    - \usetikzlibrary{arrows}
vignette: >
  %\VignetteIndexEntry{spatialcluster}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r pkg-load, echo = FALSE, message = FALSE}
devtools::load_all (".", export_all = FALSE)
#library (spatialcluster)
```

# Introduction

`spatialcluster` is an **R** package for performing spatially-constrained
clustering. Spatially-constrained clustering is a distinct mode of clustering in
which data include additional spatial coordinates in addition to the data used
for clustering, and the clustering is performed such that only spatially
contiguous or adjacent points may be merged into the same cluster (Fig. 1).

## Nomenclature

* The term "objects" is used here to refer to the objects which are to be
  aggregated into clusters; these may be points, lines, polygons, or any other
  spatial or non-spatial entities
* The term "non-spatial data" also encompasses data which are not necessarily
  spatial, but which may include some spatial component.

# Spatial clustering versus spatially-constrained clustering

## Spatial clustering


Spatial clustering is a very well-studied field [see reviews in @Han2001;
@Duque2007; @Lu2009], with many methods implemented in the **R** language (see
the CRAN Task View on [Analysis of Spatial
Data](https://cran.r-project.org/web/views/Spatial.html)). Spatial clustering
algorithms take as input a set of spatial distances between objects, and seek to
cluster those objects based on these exclusively spatial distances alone (Fig.
1A). Other non-spatial data may be included, but must be somehow reconciled with
the spatial component. This is often achieved through weighted addition to
attain approximately "spatialized" data. Figure 1B-C illustrate two related
non-spatial (B) and spatial (C) datasets. The primary data of interest depicted
in B can be "spatialized" through additively combining the associated distance
matrices of non-spatial and spatial data, and submitting the resultant distance
matrix to a clustering routine of choice.


```{r, fig.width = 12, fig.height = 4, echo = FALSE, fig.cap = "Figure 1: (A) Illustration of typical spatial clustering application for which input data are explicit spatial distances between points; (B) Illustration of clustering in some other, non-spatial dimensions, D1 and D2, for which associated spatial data in (C) do not manifest clear spatial clusters."}
getdat <- function (ncl = 5, noise = 0.1) {
    sizes <- ceiling (runif (ncl) * 20)
    x <- rep (runif (ncl), times = sizes) + runif (sum (sizes), -1, 1) * noise
    y <- rep (runif (ncl), times = sizes) + runif (sum (sizes), -1, 1) * noise
    cols <- rep (rainbow (ncl), times = sizes)
    data.frame (x = x, y = y, col = cols)
}
#layout (matrix (c (1, 2, 1, 3), 2, 2, byrow = TRUE))
par (mfrow = c (1, 3))
par (mar = c (2.0, 2.0, 1.5, 0.5), mgp = c (1, 0.7, 0))

set.seed (3)
dat <- getdat (ncl = 5, noise = 0.1)
plot (dat$x, dat$y, col = dat$col, cex = 2,
      xlab = "x", ylab = "y", xaxt = "n", yaxt = "n", main = "A")

dat <- getdat (ncl = 5, noise = 0.1)
plot (dat$x, dat$y, col = dat$col, cex = 2,
      xlab = "D1", ylab = "D2", xaxt = "n", yaxt = "n", main = "B")
dat <- getdat (ncl = 5, noise = 0.5)
plot (dat$x, dat$y, col = dat$col, cex = 2,
      xlab = "x", ylab = "y", xaxt = "n", yaxt = "n", main = "C")
```

For example, the following code illustrates the use of the `DBSCAN` algorithm
(**D**ensity **B**ased **S**patial **C**lustering of
**A**pplications with **N**oise) from the **R** package
[`dbscan`](https://cran.r-project.org/package=dbscan).
```{r, echo = FALSE}
set.seed (3)
dat_nospace <- getdat (ncl = 5, noise = 0.1)
dat_space <- getdat (ncl = 5, noise = 0.2)
nr <- min (c (nrow (dat_nospace), nrow (dat_space)))
dat_nospace <- dat_nospace [1:nr, ]
dat_space <- dat_space [1:nr, ]
d_nospace <- dist (dat_nospace [, 1:2])
d_space <- dist (dat_space [, 1:2])
d <- d_nospace + d_space
```
```{r, eval = FALSE}
d_nospace <- dist (dat_nospace) # matrix of non-spatial data
d_space <- dist (dat_space) # 2-column matrix of spatial data
d <- dat_nospace + d_space # simple linear addition
```
```{r}
library (dbscan)
db <- dbscan::dbscan (d, eps = 0.4) # more on the eps parameter below
db
```
The result shows the appropriate number of five clusters. Further insight can be
gained through visually inspecting the clusters in both the non-spatial and
spatial domains. Doing so reveals (Fig. 2) that the clustering is actually quite
representative, being clearly distinct in the non-spatial domain, and also
reasonably distinct in the spatial domain.
```{r, echo = FALSE, fig.width = 8, fig.height = 4, fig.cap = "Figure 2: (A) Non-spatial data coloured by dbscan clustering results; (B) Corresponding spatial data coloured by dbscan clustering results."}
cols <- rainbow (length (unique (db$cluster))) [db$cluster + 1] # 0-indexed
par (mfrow = c (1, 2))
par (mar = c (2.0, 2.0, 1.5, 0.5), mgp = c (1, 0.7, 0))
plot (dat_nospace$x, dat_nospace$y, cex = 1, col = cols,
      xlab = "D1", ylab = "D2", xaxt = "n", yaxt = "n", main = "non-spatial")
plot (dat_space$x, dat_space$y, cex = 1, col = cols,
      xlab = "x", ylab = "y", xaxt = "n", yaxt = "n", main = "spatial")
```

These results demonstrate that reasonable results can indeed be obtained through
simple linear combination of non-spatial and spatial distances. This approach is
very simple, and it is very easy to submit such combined distance matrices to
high-performance clustering routines such as
[`dbscan`](https://cran.r-project.org/package=dbscan). There are nevertheless
two notable shortcomings:

1. There is no objectively best way to combine non-spatial and spatial distance
   matrices; and
2. Routines such as [`dbscan`](https://cran.r-project.org/package=dbscan) still
   require an effectively arbitrary parameter represented by the above value of
   `eps = 0.4`. This value was simply chosen to best reflect the known structure
   of the input clusters, but in any practical application will remain largely
   arbitrary.

Even one of the most recent **R** packages dedicated to spatial clustering
[@Chavent2017] follows precisely this linear addition strategy, via a parameter
determining the relative weights of the non-spatial and spatial data.

We assert here that such approaches provide more a means of attaining
approximately spatially-structured clustering schemes, rather than actually
providing spatially-constrained clusters in the sense we now explore.

# Spatially-constrained clustering

The [`spatialcluster`
package](https://github.com/mpadge/spatialcluster) performs strict *spatially
constrained clustering*. This means that clusters are formed entirely on the
basis of the non-spatial data, while the spatial data provide a constraint used
to ensure that all clusters are spatially contiguous.

## The SKATER algorithm

One of the most widespread algorithms for spatially constrained clustering is
the so-called "SKATER" algorithm [**S**patial **K**luster **A**nalysis by
**T**ree **E**dge **R**emoval, @Assuncao2006], available via the **R** package
[`spdep`](https://cran.r-project.org/package=spdep) [@Bivand2013; @Bivand2015].
This algorithm constructs a minimum spanning tree (MST) connecting neighbours
defined by the spatial data, with all connections defined by minimal distances
in the non-spatial data. The SKATER algorithm constructs the simplest of all
MSTs, by iterating through the list of neighbouring edges according to
increasing (non-spatial) distance, and inserting edges where these connect
previously unconnected objects.  The resultant MST is then partitioned into a
specified number of clusters such that the intra-cluster sum of squared
deviations from the mean (of the non-spatial data) is minimised. Further details
are given in @Assuncao2006.

## The REDCAP algorithms

The REDCAP algorithms for spatially-constrained clustering [**RE**gionalization
with **D**ynamically **C**onstrained **A**gglomerative **C**lustering, @Guo2008]
employs three distinct methods for constructing MSTs. The original algorithms
actually develop these three methods for two distinct ways of constructing
spanning trees: through using nearest neighbours only ("first-order
constraint"), or through considering all neighbours of each object ("full-order
constraint"). The results of the cited manuscript clearly reveal the superiority
of the latter, and only full-order constraints are considered here.

The three methods refer to methods for determining which edges are selected to
link clusters, via either single-, average-, or complete-linkage, which function
as follows:

1. Single-linkage clustering simply loops through the list of minimal-distance,
   nearest-neighbour edges, and inserts each next-minima distance edge into the
   clustering tree if it is not part of any previous cluster, and if it connects
   two separate yet contiguous clusters.
2. Average-linkage clustering assigns weights to each unassigned edge based on their
   average distance to all edges within all adjacent clusters.  Thus when an
   edge becomes part of a cluster, the distances to all non-assigned edges
   adjacent to that cluster are updated to reflect the change in average
   distance calculated over all edges in that cluster. Edges are continually
   (re-)sorted based on average distances, and the tree is built through
   sequentially inserting minimal-distance edges.
3. Maximal-linkage clustering forms clusters through inserting the edge having
   the minimal distance to the farthest point of any adjacent cluster.

Single-linkage is equivalent to the SKATER algorithm, where the single best edge
connecting two clusters is selected. (The SKATER algorithm is actually
equivalent to the worst-performing REDCAP algorithm: single-linkage,
first-order constraint.) [See @Guo2008 for details.] The resultant MSTs are then
partitioned into specified numbers of clusters using an identical approach to
SKATER, namely through minimising the intra-cluster sum of squared deviations
from mean values.



# The Exact Clustering Algorithm

As described above, the REDCAP algorithms provide different ways of constructing
minimal spanning trees for a given data set, with resultant clusters based on
bisecting these spanning trees. They effectively represent the relationships
within a data set by the "best" minimal set (according to the chosen algorithm).
Here, we develop a simple algorithm for deriving a clustering scheme that uses
the full set of nearest-neighbour relationships in a given data set.

Each point or object within a typical planar (effectively two-dimensional) data
set may have on average just under three nearest neighbours if these are
calculated with a triangulation, or potentially up to $k$ for some $k$-nearest
neighbours scheme. Each point in a minimal-spanning tree generally has an
average of between two and three neighbours (one for terminal nodes; two for
non-branching nodes; three for branching nodes). It may accordingly be presumed
that reducing a full set of neighbours to an MST reduces average numbers of
edges per node from $\gtrapprox 3$ to $\sim 2.5$. The loss in computational
complexity produced through using all neighbouring nodes instead of an MST is
thus likely to be only around 20%. Moreover, if a clustering algorithm scales
sub-linearly with $N$, as many do, this loss is likely to be even less
pronounced with increasing $N$.

The exact clustering algorithm proceeds through the following steps, looping
until all data objects have been allocated to a cluster, and beginning with both
`i = 0` and `clnum = 0`. The primary data is an array of edges (`edge`) sorted
by increasing distance.

```{r, eval = FALSE}
1.  Select edge [i] connecting nodes a and b.
2.  i++
3.  if both a and b are already in clusters:
        continue to next iteration
    else if neither node is in a cluster:
        cluster (clnum) = create_new_cluster (c (a, b))
        clnum++
    else if only a is in a cluster:
        set_cluster_number (a, get_cluster_number (b))
    else if only b is in a cluster:
        set_cluster_number (b, get_cluster_number (a))
```

This procedure allocates all nodes (data objects) to clusters. The number of
resultant clusters can not be known in advance, and many clusters may be very
small, comprising for example only a single edge connecting two nodes. This
initial clustering may then be used as the basis of a further hierarchical
clustering, through sequentially merging clusters having the minimal distance
according to some specified metric. This merging involves linking previously
distinct clusters according to some specified criteria, for which the same three
criteria used in the REDCAP algorithms can be directly applied here. Note,
however, that average and maximal distances in the REDCAP algorithms simply
represent the respective average and maximal individual edge distances, and not
the corresponding distances traversed within a cluster through the MST.

It is possible within the present exact clustering approach to select edges
based on actual average or maximal traversal distances from a potential
connecting edge to all other edges in two clusters. Doing so, however, simply
merges the two smallest clusters, all other things being equal, because these
must by definition have the shortest average and maximal distance. Edge
selection is therefore implemented here in the same way as the REDCAP
algorithms, by selecting edges based on average and maximal single edge
distances. Clusters are hierarchically connected by selecting edges according to
on of the following three schemes:

1. Single-linkage: select the single edge having the minimal distance between
   any two clusters;
2. Average-linkage: select the edge connecting the two clusters which, when
   merged, give the lowest average intra-cluster edge distance; or
3. Maximal-linkage: select the edge connecting the two clusters which, when
   merged, gives the lowest maximal intra-cluster edge distance.

The latter two of these yield clusters with preferentially shorter intra-cluster
distances, yet the selection procedure remains statistically unbiased by cluster
size.

# Clustering Origin-Destination (OD) matrices

The exact clustering scheme provides a uniquely powerful approach to discerning
spatial clusters in Origin-Destination (OD) matrices. These are ubiquitous in
transport planning, and quantify numbers or densities of journeys undertaken
between a set of origin points and a (potentially different) set of destination
points. Origin-destination matrices are frequently modelled by spatial
interaction models, which explain the "interaction" between two locations based
on their size, measured as aggregate numbers or densities of trips to and from
those points, and their distance apart. The sizes of origin locations are the
row sums of the OD matrix; the sizes of destinations the column sums.
Respectively denoting the sizes of origin $i$ and destination $j$ by $O_i$ and
$D_j$, the canonical spatial interaction model is,
\begin{equation}
    S_{ij} = \frac{O_i D_j e ^ {-\alpha d_{ij}}} {\sum_m O_m \sum_n D_n}
\end{equation}
where $d_{ij}$ denotes the distance between the points $i$ and $j$.

A spatial interaction model explains the portion of the OD matrix expected to
arise based on individual locational importance and relative position alone. A
spatial interaction model represents the portion of an OD matrix able to be
explained by the hypotheses that (i) "larger" or more important locations must
be expected to attract more journeys, and (ii) journeys between two locations
must be expected to decrease with increasing distance, all other things being
equal. 

We develop here an approach to extracting that portion of an OD matrix
reflecting processes beyond those expected from simple spatial interactions
alone. This is not done here in terms of numbers or densities of trips, because
results would then still depend on the original scales of observations -
"larger" locations would still manifest larger anomalies.\footnote{But
covariances also suffer this same scaling issue!} Instead, OD matrices are
converted to covariance matrices, through calculating row- and column-wise
covariances, respectively representing the covariances of origin and destination
locations. Resultant covariances can either be stored in the diagonal halves of
two separate matrices, or combined into a single, non-symmetrical matrix.

Covariance matrices calculated from spatial interaction models can be subtracted
from covariance matrices calculated from directly observed data to quantify
aggregate covariance beyond that expected from spatial interaction models alone.
Standardised measures are derived here as relative deviations, $(C_{obs} -
C_{SI}) / C_{SI}$.  We use these resultant covariance matrices to discern
spatial clusters within which observed interactions are anomalously high. An
immediate advantage of this approach is that all pair-wise interactions for
which observed covariances are merely equal to or less than those explained by
spatial interaction may be removed (through replacing with `NA` or `NaN`
values), so that clustering uses only those interactions which exceed neutrally
expected values.

Finally, note that covariances are the obverse of distances, and so need to be
converted to some suitable distance metric such as ${\rm max} C - C_{ij}$. The
resultant matrix can then be used to extract a set of (triangulated or otherwise
nearest) neighbours which can be used to extract exact clusters. In summary, an
OD matrix can be used to form a set of neighbouring edges through the following
steps:

1. Fit a spatial interaction model and derive corresponding model OD matrix;
2. Calculate covariance matrices for both observed and model OD matrices, and
   for either or both origins and/or destinations;
3. Subtract modelled from observed covariance matrix to obtain standardised
   covariance fractions beyond those expected from spatial interaction alone;
4. (Optionally) Set all fractions $\le 0$ to `NA`.
5. Convert to distance metric, for example, $C_{ij} \rightarrow {\rm max} C -
   C_{ij}$.
6. Construct set of neighbouring edges via preferred algorithm (such as
   triangulation or $k$-nearest neighbours), and including only those neighbours
   with non-`NA` distances.
7. Use resultant edges to calculate exact clusters using chosen linkage scheme.


# References
